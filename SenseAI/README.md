SenseAI ğŸ¤–ğŸ‘ï¸

Bridging Vision with AI-Powered Assistance for Visually Impaired Individuals
SenseAI is an innovative, AI-driven assistive application designed to enhance the independence and quality of life of visually impaired individuals. By leveraging advanced AI technologies, the project provides real-time scene understanding, text-to-speech conversion, object detection, and personalized assistance, enabling users to interact with their environment confidently and efficiently.

ğŸš€ Features

ğŸ” Real-Time Scene Understanding
Describes uploaded images with rich, context-aware text generated by Google Generative AI (Gemini-1.5 Flash).
Automatically converts text descriptions to audio using pyttsx3 for easy accessibility.

ğŸ—£ï¸ Text-to-Speech Conversion for Visual Content
Extracts textual information from images using pytesseract (OCR) and reads it aloud.
Supports offline audio playback, ensuring reliable usage.

ğŸš§ Object and Obstacle Detection
Identifies objects and obstacles using the YOLOv8 model.
Visual and textual outputs assist users in understanding their surroundings.

ğŸ› ï¸ Personalized Assistance for Daily Tasks
Combines image descriptions and user queries to provide tailored solutions using Google Generative AI.
Delivers outputs in both text and speech formats.


ğŸ› ï¸ Tools and Technologies
Streamlit: Interactive web interface for user interactions.

Google Generative AI: Generates descriptive and personalized responses for scene analysis.

YOLOv8: State-of-the-art object detection for identifying items and obstacles.

pytesseract: Extracts text from images for OCR-based features.

pyttsx3: Provides offline text-to-speech capabilities.

Python: Backend programming language for seamless integration and processing.


Select features to activate from the sidebar.
Upload an image for analysis.
Enjoy AI-powered assistance tailored for your needs.

ğŸ¯ Use Cases
Scene Description: Understand surroundings through image descriptions.

Text-to-Speech: Read text from menus, books, or labels.

Object Detection: Identify objects and obstacles in unfamiliar environments.

Personalized Queries: Get custom assistance for specific tasks.

ğŸŒ± Future Enhancements
Voice command integration for hands-free interaction.
Mobile app deployment for better portability.
Real-time video analysis for dynamic obstacle detection.
Multilingual support in text-to-speech and OCR features.

ğŸ¤ Acknowledgments
I express my heartfelt gratitude to Innomatics Research Labs and Kanav Bansal for their guidance and support throughout this project.

