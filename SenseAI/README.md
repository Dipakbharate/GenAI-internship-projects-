SenseAI 🤖👁️

Bridging Vision with AI-Powered Assistance for Visually Impaired Individuals
SenseAI is an innovative, AI-driven assistive application designed to enhance the independence and quality of life of visually impaired individuals. By leveraging advanced AI technologies, the project provides real-time scene understanding, text-to-speech conversion, object detection, and personalized assistance, enabling users to interact with their environment confidently and efficiently.

🚀 Features

🔍 Real-Time Scene Understanding
Describes uploaded images with rich, context-aware text generated by Google Generative AI (Gemini-1.5 Flash).
Automatically converts text descriptions to audio using pyttsx3 for easy accessibility.

🗣️ Text-to-Speech Conversion for Visual Content
Extracts textual information from images using pytesseract (OCR) and reads it aloud.
Supports offline audio playback, ensuring reliable usage.

🚧 Object and Obstacle Detection
Identifies objects and obstacles using the YOLOv8 model.
Visual and textual outputs assist users in understanding their surroundings.

🛠️ Personalized Assistance for Daily Tasks
Combines image descriptions and user queries to provide tailored solutions using Google Generative AI.
Delivers outputs in both text and speech formats.


🛠️ Tools and Technologies
Streamlit: Interactive web interface for user interactions.

Google Generative AI: Generates descriptive and personalized responses for scene analysis.

YOLOv8: State-of-the-art object detection for identifying items and obstacles.

pytesseract: Extracts text from images for OCR-based features.

pyttsx3: Provides offline text-to-speech capabilities.

Python: Backend programming language for seamless integration and processing.


Select features to activate from the sidebar.
Upload an image for analysis.
Enjoy AI-powered assistance tailored for your needs.

🎯 Use Cases
Scene Description: Understand surroundings through image descriptions.

Text-to-Speech: Read text from menus, books, or labels.

Object Detection: Identify objects and obstacles in unfamiliar environments.

Personalized Queries: Get custom assistance for specific tasks.

🌱 Future Enhancements
Voice command integration for hands-free interaction.
Mobile app deployment for better portability.
Real-time video analysis for dynamic obstacle detection.
Multilingual support in text-to-speech and OCR features.

🤝 Acknowledgments
I express my heartfelt gratitude to Innomatics Research Labs and Kanav Bansal for their guidance and support throughout this project.

